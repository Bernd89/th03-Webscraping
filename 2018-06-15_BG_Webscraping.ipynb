{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping mit Scrapy\n",
    "\n",
    "\n",
    "## 1. Theoretische Grundlagen\n",
    "\n",
    "Webscraping bezeichnet verschiedene Methoden, um Informationen aus dem Internet zu sammeln. Im Allgemeinen wird dies mit einer Software durchgeführt, die menschliches Websurfen simuliert, um bestimmte Informationen von verschiedenen Webseiten zu sammeln. Diese Programme werden Webscraper genannt. Weiterhin wird Webscraping auch als Web Data Extraction, Screen Scraping oder Web Harvesting bezeichnet[1]. Ziel ist die Informationsgewinnung und das Extrahieren von Daten[12].\n",
    "\n",
    "Webscraper laden Webseiten in den Arbeitsspeicher, suchen dort nach bestimmten Mustern und Informationen und „kratzen“ diese aus dem Quelltext heraus. Damit ähnelt die Funktionsweise der von Webcrawlern für Suchmaschinen. Im Unterschied zu diesen ist das Ziel eines Webscrapers nicht die Relevanz einer Webseite zu beurteilen, sondern nach bestimmten Informationen zu suchen und diese in ein strukturiertes Format zu übersetzen. Scraper nutzen für die Lokalisierung von Informationen die hierarchische Struktur von HTML-Dokumenten[8]. Das Grundgerüst eines HTML-Dokumente zeigt Abildung eins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/html.jpg) <center> Abbildung 1: HTML-Grundgerüst [9] </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über standardisierte Abfragesprachen, wie XPath und CSS-Selektoren, kann das Skript dann auf einzelne Elemente zugreifen und diese weiterverarbeiten[8]. Den Ablauf einer Extraktion von Informationen einer Webseite zeigt Abbildung 2.\n",
    "\n",
    "<img src=\"img/scraping.jpg\">\n",
    "<center> Abbildung 2: Funktionsweise  eines einfachen Webscrapers[8] </center>\n",
    "\n",
    "Jetzt kennen wir also den Ablauf eines Scrapers. Doch was sind nun die bereits erwähnten CSS-Selektoren und XPaths?\n",
    "\n",
    "### CSS-Selectoren\n",
    "\n",
    "CSS-Selektoren sind Muster, die zum Auswählen von Elementen verwendet werden. Hier sind einige Beispiele für übliche Selektoren[2]:\n",
    "\n",
    "- Select any tag: *\n",
    "- Select by tag a: a\n",
    "- Select by class of \"link\": .link\n",
    "- Select by tag a with class \"link\": a.link\n",
    "- Select by tag a with ID \"home\": a#home\n",
    "- Select by child span of tag a: a > span\n",
    "\n",
    "### XPath\n",
    "\n",
    "Die XML Path Language (XPath) dient dem Adressieren von Knoten in XML-Dokumenten und benutzt hierzu pfadähnliche Ausdrücke[10]. Neben XML kann sie auch für HTML verwendet werden[11]. Weiterhin können mit XPath sowohl Element- als auch Attributknoten adressiert werden[10].\n",
    "\n",
    "<img src=\"img/elementknoten.jpg\">\n",
    "<center> Abbildung 3: Adressierung des Elementknotens[10] </center>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/attributknoten.jpg\">\n",
    "<center> Abbildung 4: Adressierung des Attributknotens[10] </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorteile\n",
    "\n",
    "Durch Scraping müssen Informationen nicht mehr umständlich von mehreren Seiten vom Nutzer selber zusammengetragen und verglichen werden. Besonders Google setzt auf Scraping, um Informationen übersichtlich für den Nutzer darzustellen. So lassen sich zum Beispiel Preise auf einen Blick bei Google Shopping vergleichen. Weiterhin ist es bei einigen Suchanfragen möglich, seine Antwort zu bekommen, ohne auf ein einziges Suchergebnis zu klicken. Sucht man beispielsweise nach “Wetter” so erscheinen alle notwendigen Informationen bei Google selber, wodurch dem User der Besuch einer Webseite erspart wird [13]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gefahren\n",
    "\n",
    "Eine Gefahr im Scraping besteht im Auslesen von sensiblen Daten, wie Telefonnummern, Adressen oder E-Mail Adressen, die dann zu Spamzwecken ausgenutzt werden[13]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verwendung\n",
    "\n",
    "Scraping wird für viele Einsatzwecke verwendet[14]:\n",
    "- Web-Analyse-Tools rufen Platzierungen bei Google und anderen Suchmaschinen ab und bereiten diese Daten für ihre Kunden auf.\n",
    "- RSS-Dienste: hierbei werden über RSS-Feeds bereitgestellte Inhalte auf anderen Websites verwendet\n",
    "- Wetterdaten: viele Websites wie Reiseportale nutzen Wetterdaten von großen Meteo-Seiten, um ihre eigene Funktionalität zu erhöhen\n",
    "- Fahr- und Flugpläne: so nutzt u.a. Google relevante Daten von der Bahn, um die Reiseplanfunktion in Google Maps zu ergänzen\n",
    "- Vergleichsportale\n",
    "- Vergleich der Preise mit denen der Konkurenz[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechtliches\n",
    "\n",
    "- Datenbanken sind nach den §§ 87b Abs. 1 Satz 2 UrhG gegen die öffentliche Wiedergabe eines wesentlichen Teils geschützt. Doch wann extrahiert und veröffentlicht die Vergleichsseite einen „wesentlichen Teil“?\n",
    "- Hinsichtlich der Inhalte der ausgelesenen Webseite könnte darüber hinaus sogar ein Urheberrecht bestehen – so zum Beispiel bei der individuellen Bewertung von Restaurants. In dieses würde eingegriffen werden, wenn man tatsächlich urheberschutzfähige Texte extrahiert und als solche in die eigene Seite einbindet[15].\n",
    "\n",
    "#### Urteile zu Webscraping\n",
    "\n",
    "Fall 1: Flugtickets\n",
    "\n",
    "Das OLG Frankfurt a.M. (Urteil vom 5.3.09 – 6 U 221/08) und das OLG Hamburg (Urteil vom 24.10.12 – 5 U 38/10) haben entschieden, dass die Vermittlung von Flugtickets durch das Screen Scraping keine unzulässige Nutzung nach § 87 b Abs. 1 Satz 2 UrhG ist, da die Nutzung von Datensätzen einzelner Flugverbindungen sich im Rahmen der normalen Auswertung der Datenbank halte und keine „wesentlichen Teile“ der Datenbank veröffentlicht werden.\n",
    "\n",
    "\n",
    "Fall 2: Automobilbörsen\n",
    "\n",
    "In diesen Fällen ging es um den Vertrieb einer Software, die es ermöglicht innerhalb von kurzen Zeitabständen Suchanfragen bei mehreren Internet-Autobörsen gleichzeitig durchzuführen um nach bestimmten Angeboten zu suchen, ohne dass die Nutzer noch auf die Seiten der Autobörsen selbst gehen müssen.\n",
    "\n",
    "- Das OLG Hamburg hat dazu entschieden (Urteil vom 16.4.09 – 5 U 101/08), dass der Suchdienst keine unzumutbare Beeinträchtigung der berechtigten Interessen der Autobörsenbetreiber darstellt.\n",
    "- Der BGH hat in dieselbe Richtung argumentiert (Urteil vom 22.6.11 – I ZR 159/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Webscraping mit Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Möglichkeit für das webscraping bietet das Python-Framework Scrapy, welches über den folgenden Befehl installiert werden kann[2]:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Installation können wir mit dem Befehl startprojekt ein neues Projekt anlegen.\n",
    "Dazu navigiert man in der Komandozeile in das Verzeichnis, in dem das Projekt angelegt werden soll[2]. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy startproject example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dadurch werden die folgende Dateien generiert:\n",
    "\n",
    "scrapy.cfg\n",
    "example/\n",
    "    __init__.py\n",
    "    items.py\n",
    "    pipelines.py\n",
    "    settings.py\n",
    "    spiders/\n",
    "        __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die wichtigsten Dateien sind:\n",
    "- items.py: Diese Datei definiert ein Modell der Felder, die gescraped werden.\n",
    "- settings.py: Diese Datei definiert Einstellungen wie den Benutzeragenten und die Verzögerung zwischen den Downloadanfragen.\n",
    "- spiders: In diesem Ordner wird der eigentliche Scraping- und Crawling-Code gespeichert.\n",
    "\n",
    "Die jeweiligen Klassen können aber auch manuel in einem Notebook angelegt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns zunächst die Datei items an. Diese sieht standartmäßig so aus[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ExampleItem(scrapy.Item):\n",
    "# define the fields for your item here like:\n",
    "# name = scrapy.Field()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unser Beispiel sollen Daten der Seite [example.webscraping.com](http://example.webscraping.com) gesammelt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/website.png\">\n",
    "<center> Abbildung 5: Beispielseite[16] </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse Items könnte daher beispielsweise so aussehen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ExampleItem(scrapy.Item):\n",
    "# define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    area = scrapy.Field()\n",
    "    population = scrapy.Field()\n",
    "    capital = scrapy.Field()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns als nächstes den Ordner Spiders an. Spider sind Klassen, die Scrapy verwendet, um Informationen von Webseiten zu scrappen. Sie werden von der Klasse scrapy.Spider abgeleitet und definieren die Anforderungen[2]. Für unser Beispiel wird die Klasse CountrySpider angelegt und die Tabelle mit den Ländern extrahiert. Zum Verständnis sollen die Daten nicht gespeichert sondern einfach ausgegeben werden. Dazu haben wir die in der Theorie genannten Möglichkeiten:\n",
    "- CSS\n",
    "- XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Möglichkeit 1: CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Afghanista'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Aland-Isla'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Albania-3\"'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Algeria-4\"'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/American-S'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Andorra-6\"'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Angola-7\">'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Anguilla-8'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Antarctica'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Antigua-an'>]\n",
      "\n",
      "[<Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Afghanistan'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Aland Islands'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Albania'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Algeria'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' American Samoa'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Andorra'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Angola'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Anguilla'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Antarctica'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Antigua and Barbuda'>]\n",
      "\n",
      "[' Afghanistan', ' Aland Islands', ' Albania', ' Algeria', ' American Samoa', ' Andorra', ' Angola', ' Anguilla', ' Antarctica', ' Antigua and Barbuda']\n",
      "\n",
      " Afghanistan\n",
      "\n",
      " Afghanistan\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Ausgabe der SelectorList\n",
    "        print(response.css('div#results div a'))\n",
    "        print('')\n",
    "        # Ausgabe der SelectorList mit Text\n",
    "        print(response.css('div#results div a::text'))\n",
    "        print('')\n",
    "        # Ausgabe der Tabelle als Liste\n",
    "        print(response.css('div#results div a::text').extract())\n",
    "        print('')\n",
    "        # Ausgabe des ersten Elements\n",
    "        print(response.css('div#results div a::text')[0].extract())\n",
    "        print('')\n",
    "        print(response.css('div#results div a::text').extract_first())\n",
    "   \n",
    "\n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erläuterungen:\n",
    "- name: String zur Identifizierung der Spider[2].\n",
    "- start_urls: Dieses Attribut ist eine Liste von URLs zum Starten des Spiders[2].\n",
    "- allowed_domains: Dieses Attribut ist eine Liste der Domänen, die gecrawlt werden können. Wenn dies nicht definiert ist, kann jede Domäne gecrawlt werden[2].\n",
    "- parse: Dies ist die Standardmethode, die von Scrapy verwendet wird, um heruntergeladene Daten zu verarbeiten[4].\n",
    "- CrawlerRunner:Diese Klasse ist ein dünner Wrapper, der einige einfache Helfer kapselt, um mehrere Crawler laufen zu lassen [5].\n",
    "- reactor: Der Reaktor ist der Kern der Ereignisschleife. Die Ereignisschleife ist ein Programmierkonstrukt, das auf Ereignisse oder Nachrichten in einem Programm wartet und diese versendet[7]. Leider kann er nur einmal gestartet werden sonst erhält man den Fehler \"ReactorNotRestartable\".\n",
    "\n",
    "Der Crawler kann aber auch über die Konsole mit fogendem Befehl gestartet werden"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy crawl country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möchte man die Ergebnisse in einer Datei speichern startet man den Crawler folgendermaßen[3]:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy crawl country -o test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse werden also in einer json-Datei gespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Programm gibt jedoch nur die Länder der ersten Seite aus. Möchte man die Länder aller Seiten ausgeben. Muss das Programm folgendermaßen ergänzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Afghanistan', ' Aland Islands', ' Albania', ' Algeria', ' American Samoa', ' Andorra', ' Angola', ' Anguilla', ' Antarctica', ' Antigua and Barbuda']\n",
      "\n",
      "[' Argentina', ' Armenia', ' Aruba', ' Australia', ' Austria', ' Azerbaijan', ' Bahamas', ' Bahrain', ' Bangladesh', ' Barbados']\n",
      "\n",
      "[' Belarus', ' Belgium', ' Belize', ' Benin', ' Bermuda', ' Bhutan', ' Bolivia', ' Bonaire, Saint Eustatius and Saba', ' Bosnia and Herzegovina', ' Botswana']\n",
      "\n",
      "[' Bouvet Island', ' Brazil', ' British Indian Ocean Territory', ' British Virgin Islands', ' Brunei', ' Bulgaria', ' Burkina Faso', ' Burundi', ' Cambodia', ' Cameroon']\n",
      "\n",
      "[' Canada', ' Cape Verde', ' Cayman Islands', ' Central African Republic', ' Chad', ' Chile', ' China', ' Christmas Island', ' Cocos Islands', ' Colombia']\n",
      "\n",
      "[' Comoros', ' Cook Islands', ' Costa Rica', ' Croatia', ' Cuba', ' Curacao', ' Cyprus', ' Czech Republic', ' Democratic Republic of the Congo', ' Denmark']\n",
      "\n",
      "[' Djibouti', ' Dominica', ' Dominican Republic', ' East Timor', ' Ecuador', ' Egypt', ' El Salvador', ' Equatorial Guinea', ' Eritrea', ' Estonia']\n",
      "\n",
      "[' Ethiopia', ' Falkland Islands', ' Faroe Islands', ' Fiji', ' Finland', ' France', ' French Guiana', ' French Polynesia', ' French Southern Territories', ' Gabon']\n",
      "\n",
      "[' Gambia', ' Georgia', ' Germany', ' Ghana', ' Gibraltar', ' Greece', ' Greenland', ' Grenada', ' Guadeloupe', ' Guam']\n",
      "\n",
      "[' Guatemala', ' Guernsey', ' Guinea', ' Guinea-Bissau', ' Guyana', ' Haiti', ' Heard Island and McDonald Islands', ' Honduras', ' Hong Kong', ' Hungary']\n",
      "\n",
      "[' Iceland', ' India', ' Indonesia', ' Iran', ' Iraq', ' Ireland', ' Isle of Man', ' Israel', ' Italy', ' Ivory Coast']\n",
      "\n",
      "[' Jamaica', ' Japan', ' Jersey', ' Jordan', ' Kazakhstan', ' Kenya', ' Kiribati', ' Kosovo', ' Kuwait', ' Kyrgyzstan']\n",
      "\n",
      "[' Laos', ' Latvia', ' Lebanon', ' Lesotho', ' Liberia', ' Libya', ' Liechtenstein', ' Lithuania', ' Luxembourg', ' Macao']\n",
      "\n",
      "[' Macedonia', ' Madagascar', ' Malawi', ' Malaysia', ' Maldives', ' Mali', ' Malta', ' Marshall Islands', ' Martinique', ' Mauritania']\n",
      "\n",
      "[' Mauritius', ' Mayotte', ' Mexico', ' Micronesia', ' Moldova', ' Monaco', ' Mongolia', ' Montenegro', ' Montserrat', ' Morocco']\n",
      "\n",
      "[' Mozambique', ' Myanmar', ' Namibia', ' Nauru', ' Nepal', ' Netherlands', ' Netherlands Antilles', ' New Caledonia', ' New Zealand', ' Nicaragua']\n",
      "\n",
      "[' Niger', ' Nigeria', ' Niue', ' Norfolk Island', ' North Korea', ' Northern Mariana Islands', ' Norway', ' Oman', ' Pakistan', ' Palau']\n",
      "\n",
      "[' Palestinian Territory', ' Panama', ' Papua New Guinea', ' Paraguay', ' Peru', ' Philippines', ' Pitcairn', ' Poland', ' Portugal', ' Puerto Rico']\n",
      "\n",
      "[' Qatar', ' Republic of the Congo', ' Reunion', ' Romania', ' Russia', ' Rwanda', ' Saint Barthelemy', ' Saint Helena', ' Saint Kitts and Nevis', ' Saint Lucia']\n",
      "\n",
      "[' Saint Martin', ' Saint Pierre and Miquelon', ' Saint Vincent and the Grenadines', ' Samoa', ' San Marino', ' Sao Tome and Principe', ' Saudi Arabia', ' Senegal', ' Serbia', ' Serbia and Montenegro']\n",
      "\n",
      "[' Seychelles', ' Sierra Leone', ' Singapore', ' Sint Maarten', ' Slovakia', ' Slovenia', ' Solomon Islands', ' Somalia', ' South Africa', ' South Georgia and the South Sandwich Islands']\n",
      "\n",
      "[' South Korea', ' South Sudan', ' Spain', ' Sri Lanka', ' Sudan', ' Suriname', ' Svalbard and Jan Mayen', ' Swaziland', ' Sweden', ' Switzerland']\n",
      "\n",
      "[' Syria', ' Taiwan', ' Tajikistan', ' Tanzania', ' Thailand', ' Togo', ' Tokelau', ' Tonga', ' Trinidad and Tobago', ' Tunisia']\n",
      "\n",
      "[' Turkey', ' Turkmenistan', ' Turks and Caicos Islands', ' Tuvalu', ' U.S. Virgin Islands', ' Uganda', ' Ukraine', ' United Arab Emirates', ' United Kingdom', ' United States']\n",
      "\n",
      "[' United States Minor Outlying Islands', ' Uruguay', ' Uzbekistan', ' Vanuatu', ' Vatican', ' Venezuela', ' Vietnam', ' Wallis and Futuna', ' Western Sahara', ' Yemen']\n",
      "\n",
      "[' Zambia', ' Zimbabwe']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    # Wartezeit zwischen den Downloadanfragen\n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        print(response.css('div#results div a::text').extract())\n",
    "        print('')\n",
    "        \n",
    "        # Bei der ersten Seite ist der Link zur nächsten seite der Erste,\n",
    "        # auf den weiteren Seiten das zweite Element und der erste Link führt zur vorherigen Seite\n",
    "        if response.css('div#pagination a::text').extract_first() == \"Next >\":\n",
    "            next_page = response.css('div#pagination a::attr(href)').extract_first()\n",
    "        else:\n",
    "            next_page = response.css('div#pagination a::attr(href)')[1].extract()\n",
    "        \n",
    "        # Hier wird überprüft, ob es eine nächste Seite gibt, falls ja wird sie aufgerufen um die dortigen Daten zu sammeln.\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "   \n",
    "\n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes soll neben dem Namen der Länder auch die Fläche, Einwohnerzahl und die Hauptstadt zu den Ländern der ersten Seite ausgegeben werden. Dazu muss das obere Programm folgendermaßen abgeändert werden. Natürlich könnten wir auch die Informationen der Länder aller Seiten ausgeben, indem wir dem \"next >\"-Link folgen. Wegen der Übersicht lassen wir dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land: Antigua and Barbuda\n",
      "Fläche: 443 square kilometres\n",
      "Einwohner: 86,754\n",
      "Hauptstadt: St. John's\n",
      "\n",
      "Land: Antarctica\n",
      "Fläche: 14,000,000 square kilometres\n",
      "Einwohner: 0\n",
      "Hauptstadt: .aq\n",
      "\n",
      "Land: Anguilla\n",
      "Fläche: 102 square kilometres\n",
      "Einwohner: 13,254\n",
      "Hauptstadt: The Valley\n",
      "\n",
      "Land: Angola\n",
      "Fläche: 1,246,700 square kilometres\n",
      "Einwohner: 13,068,161\n",
      "Hauptstadt: Luanda\n",
      "\n",
      "Land: Andorra\n",
      "Fläche: 468 square kilometres\n",
      "Einwohner: 84,000\n",
      "Hauptstadt: Andorra la Vella\n",
      "\n",
      "Land: American Samoa\n",
      "Fläche: 199 square kilometres\n",
      "Einwohner: 57,881\n",
      "Hauptstadt: Pago Pago\n",
      "\n",
      "Land: Algeria\n",
      "Fläche: 2,381,740 square kilometres\n",
      "Einwohner: 34,586,184\n",
      "Hauptstadt: Algiers\n",
      "\n",
      "Land: Albania\n",
      "Fläche: 28,748 square kilometres\n",
      "Einwohner: 2,986,952\n",
      "Hauptstadt: Tirana\n",
      "\n",
      "Land: Aland Islands\n",
      "Fläche: 1,580 square kilometres\n",
      "Einwohner: 26,711\n",
      "Hauptstadt: Mariehamn\n",
      "\n",
      "Land: Afghanistan\n",
      "Fläche: 647,500 square kilometres\n",
      "Einwohner: 29,121,286\n",
      "Hauptstadt: Kabul\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Links zu den Seiten der jeweiligen Länder\n",
    "        urls = response.css('div#results div a::attr(href)').extract()\n",
    "        for url in urls:\n",
    "            url = response.urljoin(url)\n",
    "            # Aufruf der Methode zur Extraktion der Details\n",
    "            yield scrapy.Request(url=url, callback=self.parse_details)\n",
    "    \n",
    "    # Methode zum Extrahieren der aufgerufenen Seiten mit den Informationen der Länder\n",
    "    def parse_details(self, response):\n",
    "        \n",
    "        print(\"Land:\", response.css('td.w2p_fw::text')[3].extract())\n",
    "        print(\"Fläche:\", response.css('td.w2p_fw::text')[0].extract())\n",
    "        print(\"Einwohner:\", response.css('td.w2p_fw::text')[1].extract())\n",
    "        print(\"Hauptstadt:\", response.css('td.w2p_fw::text')[4].extract())\n",
    "        print('')\n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Möglichkeit 2: XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPaths bieten mehr Leistungen als CSS-Selektoren, da sie neben dem Navigieren der Struktur auch den Inhalt betrachten können. Mit XPath kann zum Beispiel der Link ausgewählt werden, der den Text \"Nächste Seite\" enthält. Dadurch wird das Scraping viel einfacher[3]. Sehen wir uns unser Beispiel noch einmal an. Diesmal soll aber XPath statt CSS genutzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land: Antigua and Barbuda\n",
      "Fläche: 443 square kilometres\n",
      "Einwohner: 86,754\n",
      "Hauptstadt: St. John's\n",
      "\n",
      "Land: Antarctica\n",
      "Fläche: 14,000,000 square kilometres\n",
      "Einwohner: 0\n",
      "Hauptstadt: .aq\n",
      "\n",
      "Land: Anguilla\n",
      "Fläche: 102 square kilometres\n",
      "Einwohner: 13,254\n",
      "Hauptstadt: The Valley\n",
      "\n",
      "Land: Angola\n",
      "Fläche: 1,246,700 square kilometres\n",
      "Einwohner: 13,068,161\n",
      "Hauptstadt: Luanda\n",
      "\n",
      "Land: Andorra\n",
      "Fläche: 468 square kilometres\n",
      "Einwohner: 84,000\n",
      "Hauptstadt: Andorra la Vella\n",
      "\n",
      "Land: American Samoa\n",
      "Fläche: 199 square kilometres\n",
      "Einwohner: 57,881\n",
      "Hauptstadt: Pago Pago\n",
      "\n",
      "Land: Algeria\n",
      "Fläche: 2,381,740 square kilometres\n",
      "Einwohner: 34,586,184\n",
      "Hauptstadt: Algiers\n",
      "\n",
      "Land: Albania\n",
      "Fläche: 28,748 square kilometres\n",
      "Einwohner: 2,986,952\n",
      "Hauptstadt: Tirana\n",
      "\n",
      "Land: Aland Islands\n",
      "Fläche: 1,580 square kilometres\n",
      "Einwohner: 26,711\n",
      "Hauptstadt: Mariehamn\n",
      "\n",
      "Land: Afghanistan\n",
      "Fläche: 647,500 square kilometres\n",
      "Einwohner: 29,121,286\n",
      "Hauptstadt: Kabul\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        urls = response.xpath('//div[@id=\"results\"]//a/@href').extract()\n",
    "        for url in urls:\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url=url, callback=self.parse_details)\n",
    "            \n",
    "    def parse_details(self, response):\n",
    "        \n",
    "        print(\"Land:\", response.xpath('//td[@class=\"w2p_fw\"]/text()')[3].extract())\n",
    "        print(\"Fläche:\", response.xpath('//*[@class=\"w2p_fw\"]/text()')[0].extract())\n",
    "        print(\"Einwohner:\", response.xpath('//td[contains(@class, \"w2p_fw\")]/text()')[1].extract())\n",
    "        print(\"Hauptstadt:\", response.xpath('//td[contains(@class, \"fw\")]/text()')[4].extract())\n",
    "        print('')\n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Sehen ist, dass man Tags bestimmter Klassen und ids suchen kann. Mit `text()` wird der Text zwischen den Tags extrahiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Fähigkeit von Scrapy ist die Interaktion mit Suchfunktionen und Extrahieren von Daten aus Dateien im json-Format[2]. Das folgende Beispiel sucht die ersten zehn Länder mit einem G im Namen und gibt die Ergebnisse im json-Format aus. Dazu nutzt es die Suchfunktion der Seite [example.webscraping.com](http://example.webscraping.com/places/default/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"records\": [{\"pretty_link\": \"<div><a href=\\\"/places/default/view/Bulgaria-36\\\"><img src=\\\"/places/static/images/flags/bg.png\\\" /> Bulgaria</a></div>\", \"country\": \"Bulgaria\", \"id\": 2185884}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Democratic-Republic-of-the-Congo-59\\\"><img src=\\\"/places/static/images/flags/cd.png\\\" /> Democratic Republic of the Congo</a></div>\", \"country\": \"Democratic Republic of the Congo\", \"id\": 2185907}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Egypt-66\\\"><img src=\\\"/places/static/images/flags/eg.png\\\" /> Egypt</a></div>\", \"country\": \"Egypt\", \"id\": 2185914}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Equatorial-Guinea-68\\\"><img src=\\\"/places/static/images/flags/gq.png\\\" /> Equatorial Guinea</a></div>\", \"country\": \"Equatorial Guinea\", \"id\": 2185916}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/French-Guiana-77\\\"><img src=\\\"/places/static/images/flags/gf.png\\\" /> French Guiana</a></div>\", \"country\": \"French Guiana\", \"id\": 2185925}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Gabon-80\\\"><img src=\\\"/places/static/images/flags/ga.png\\\" /> Gabon</a></div>\", \"country\": \"Gabon\", \"id\": 2185928}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Gambia-81\\\"><img src=\\\"/places/static/images/flags/gm.png\\\" /> Gambia</a></div>\", \"country\": \"Gambia\", \"id\": 2185929}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Georgia-82\\\"><img src=\\\"/places/static/images/flags/ge.png\\\" /> Georgia</a></div>\", \"country\": \"Georgia\", \"id\": 2185930}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Germany-83\\\"><img src=\\\"/places/static/images/flags/de.png\\\" /> Germany</a></div>\", \"country\": \"Germany\", \"id\": 2185931}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Ghana-84\\\"><img src=\\\"/places/static/images/flags/gh.png\\\" /> Ghana</a></div>\", \"country\": \"Ghana\", \"id\": 2185932}], \"num_pages\": 6, \"error\": \"\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    api_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "    start_urls = [api_url.format(1, 'G')]\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        print(response.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die gewünschten Daten daraus zu extrahieren muss json importiert werden und in der parse-Methode extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulgaria\n",
      "Democratic Republic of the Congo\n",
      "Egypt\n",
      "Equatorial Guinea\n",
      "French Guiana\n",
      "Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "import json\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    api_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "    start_urls = [api_url.format(1, 'G')]\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        for country in data['records']:\n",
    "            print(country['country'])\n",
    "        \n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen:\n",
    "- [1] https://www.techopedia.com/definition/5212/web-scraping Letzter Aufruf am 04.06.2018\n",
    "- [2] Richard Lawson. Web Scraping with Python. 2015.\n",
    "- [3] https://doc.scrapy.org/en/latest/intro/tutorial.html letzter Aufruf am 04.06.2018\n",
    "- [4] https://doc.scrapy.org/en/latest/topics/spiders.html#topics-spiders letzter Aufruf 05.06.2018\n",
    "- [5] https://doc.scrapy.org/en/latest/topics/practices.html?highlight=crawlerRunner letzter Aufruf 05.06.2018\n",
    "- [6] https://doc.scrapy.org/en/latest/topics/selectors.html#topics-selectors letzter Aufruf 05.06.2018\n",
    "- [7] https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html letzter Aufruf 05.06.2018\n",
    "- [8] http://www.vfm-online.de/weblog/wp-content/uploads/2014/01/info7-2016-2_S15-17.pdf\n",
    "- [9] https://wiki.selfhtml.org/wiki/HTML/Tutorials/HTML5-Grundgerüst\n",
    "- [10] https://wiki.selfhtml.org/wiki/XML/XSL/XPath\n",
    "- [11] https://doc.scrapy.org/en/latest/topics/selectors.html?highlight=XPath\n",
    "- [12] https://www.seo-analyse.com/seo-lexikon/s/scraping/\n",
    "- [13] https://www.advidera.com/glossar/web-scraping/#Verwendung\n",
    "- [14] https://de.ryte.com/wiki/Scraping\n",
    "- [15] https://www.wbs-law.de/urheberrecht/ist-screen-scraping-legal-39848/\n",
    "- [16] http://example.webscraping.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
