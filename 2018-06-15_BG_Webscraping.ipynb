{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping mit Scrapy\n",
    "\n",
    "Web Scraping ist im Wesentlichen eine Form des Data Mining und bezeichnet verschiedene Methoden, um Informationen aus dem Internet zu sammeln. Im Allgemeinen wird dies mit einer Software durchgeführt, die menschliches Websurfen simuliert, um bestimmte Informationen von verschiedenen Webseiten zu sammeln. Web Scraping wird auch als Web Data Extraction, Screen Scraping oder Web Harvesting bezeichnet[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Möglichkeit für das webscraping bietet das Python-Framework Scrapy, welches über den folgenden Befehl installiert werden kann[2]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Installation können wir mit dem Befehl startprojekt ein neues Projekt anlegen.\n",
    "Dazu navigiert man in der Komandozeile in das Verzeichnis, in dem das Projekt angelegt werden soll[2]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy startproject example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dadurch werden die folgende Dateien generiert:\n",
    "\n",
    "scrapy.cfg\n",
    "example/\n",
    "    __init__.py\n",
    "    items.py\n",
    "    pipelines.py\n",
    "    settings.py\n",
    "    spiders/\n",
    "        __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die wichtigsten Dateien sind:\n",
    "- items.py: Diese Datei definiert ein Modell der Felder, die gescraped werden.\n",
    "- settings.py: Diese Datei definiert Einstellungen wie den Benutzeragenten und die Verzögerung zwischen den Downloadanfragen.\n",
    "- spiders: In diesem Ordner wird der eigentliche Scraping- und Crawling-Code gespeichert.\n",
    "\n",
    "Die jeweiligen Klassen können aber auch manuel in einem Notebook angelegt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns zunächst die Datei items an. Diese sieht standartmäßig so aus[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ExampleItem(scrapy.Item):\n",
    "# define the fields for your item here like:\n",
    "# name = scrapy.Field()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unser Beispiel sollen Daten der Seite [example.webscraping.com](http://example.webscraping.com) gesammelt werden. Die Klasse items könnte daher beispielsweise so aussehen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ExampleItem(scrapy.Item):\n",
    "# define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    area = scrapy.Field()\n",
    "    population = scrapy.Field()\n",
    "    capital = scrapy.Field()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir den Ordner Spiders betrachten. Sehen wir uns zunächst an, wie die Daten extrahiert werden können. Dies geschieht z.B. über die Anweisung response.css('title'). Das Ergebnis der Anweisung ist ein listenähnliches Objekt namens SelectorList, das eine Liste von Selector-Objekten darstellt, die XML-/HTML-Elemente umschließen und weitere Abfragen. Für Abfragen gibt es zwei Möglichkeiten [3]:\n",
    "- css\n",
    "- xpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Möglichkeit 1: CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns als nächstes den Ordner Spiders an. Spider sind Klassen, die Scrapy verwendet, um Informationen von Webseiten zu scrappen. Sie werden von der Klasse scrapy.Spider abgeleitet und definieren die Anforderungen[2]. Für unser Beispiel wird die Klasse CountrySpieder angelegt und die Tabelle mit den Ländern mit Hilfe von css extrahiert. Zum Verständnis sollen die Daten nicht gespeichert sondern einfach ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Afghanista'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Aland-Isla'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Albania-3\"'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Algeria-4\"'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/American-S'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Andorra-6\"'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Angola-7\">'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Anguilla-8'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Antarctica'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a\" data='<a href=\"/places/default/view/Antigua-an'>]\n",
      "\n",
      "[<Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Afghanistan'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Aland Islands'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Albania'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Algeria'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' American Samoa'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Andorra'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Angola'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Anguilla'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Antarctica'>, <Selector xpath=\"descendant-or-self::div[@id = 'results']/descendant-or-self::*/div/descendant-or-self::*/a/text()\" data=' Antigua and Barbuda'>]\n",
      "\n",
      "[' Afghanistan', ' Aland Islands', ' Albania', ' Algeria', ' American Samoa', ' Andorra', ' Angola', ' Anguilla', ' Antarctica', ' Antigua and Barbuda']\n",
      "\n",
      " Afghanistan\n",
      "\n",
      " Afghanistan\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Ausgabe der SelectorList\n",
    "        print(response.css('div#results div a'))\n",
    "        print('')\n",
    "        # Ausgabe der SelectorList mit Text\n",
    "        print(response.css('div#results div a::text'))\n",
    "        print('')\n",
    "        # Ausgabe der Tabelle als Liste\n",
    "        print(response.css('div#results div a::text').extract())\n",
    "        print('')\n",
    "        # Ausgabe des ersten Elements\n",
    "        print(response.css('div#results div a::text')[0].extract())\n",
    "        print('')\n",
    "        print(response.css('div#results div a::text').extract_first())\n",
    "   \n",
    "\n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erläuterungen:\n",
    "- name: String zur Identifizierung der Spider[2].\n",
    "- start_urls: Dieses Attribut ist eine Liste von URLs zum Starten des Spiders[2].\n",
    "- allowed_domains: Dieses Attribut ist eine Liste der Domänen, die gecrawlt werden können. Wenn dies nicht definiert ist, kann jede Domäne gecrawlt werden[2].\n",
    "- parse: Dies ist die Standardmethode, die von Scrapy verwendet wird, um heruntergeladene Daten zu verarbeiten[4].\n",
    "- CrawlerRunner:Diese Klasse ist ein dünner Wrapper, der einige einfache Helfer kapselt, um mehrere Crawler laufen zu lassen [5].\n",
    "- reactor: Der Reaktor ist der Kern der Ereignisschleife. Die Ereignisschleife ist ein Programmierkonstrukt, das auf Ereignisse oder Nachrichten in einem Programm wartet und diese versendet[7]. Leider kann er nur einmal gestartet werden sonst erhält man den Fehler \"ReactorNotRestartable\".\n",
    "\n",
    "Der Crawler kann aber auch über die Konsole mit fogendem Befehl gestartet werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy crawl country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möchte man die Ergebnisse in einer Datei speichern startet man den Crawler folgendermaßen[3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy crawl country -o test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse werden also in einer json-Datei gespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Programm gibt jedoch nur die Länder der ersten Seite aus. Möchte man die Länder aller Seiten ausgeben. Muss das Programm folgendermaßen ergänzt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Afghanistan', ' Aland Islands', ' Albania', ' Algeria', ' American Samoa', ' Andorra', ' Angola', ' Anguilla', ' Antarctica', ' Antigua and Barbuda']\n",
      "\n",
      "[' Argentina', ' Armenia', ' Aruba', ' Australia', ' Austria', ' Azerbaijan', ' Bahamas', ' Bahrain', ' Bangladesh', ' Barbados']\n",
      "\n",
      "[' Belarus', ' Belgium', ' Belize', ' Benin', ' Bermuda', ' Bhutan', ' Bolivia', ' Bonaire, Saint Eustatius and Saba', ' Bosnia and Herzegovina', ' Botswana']\n",
      "\n",
      "[' Bouvet Island', ' Brazil', ' British Indian Ocean Territory', ' British Virgin Islands', ' Brunei', ' Bulgaria', ' Burkina Faso', ' Burundi', ' Cambodia', ' Cameroon']\n",
      "\n",
      "[' Canada', ' Cape Verde', ' Cayman Islands', ' Central African Republic', ' Chad', ' Chile', ' China', ' Christmas Island', ' Cocos Islands', ' Colombia']\n",
      "\n",
      "[' Comoros', ' Cook Islands', ' Costa Rica', ' Croatia', ' Cuba', ' Curacao', ' Cyprus', ' Czech Republic', ' Democratic Republic of the Congo', ' Denmark']\n",
      "\n",
      "[' Djibouti', ' Dominica', ' Dominican Republic', ' East Timor', ' Ecuador', ' Egypt', ' El Salvador', ' Equatorial Guinea', ' Eritrea', ' Estonia']\n",
      "\n",
      "[' Ethiopia', ' Falkland Islands', ' Faroe Islands', ' Fiji', ' Finland', ' France', ' French Guiana', ' French Polynesia', ' French Southern Territories', ' Gabon']\n",
      "\n",
      "[' Gambia', ' Georgia', ' Germany', ' Ghana', ' Gibraltar', ' Greece', ' Greenland', ' Grenada', ' Guadeloupe', ' Guam']\n",
      "\n",
      "[' Guatemala', ' Guernsey', ' Guinea', ' Guinea-Bissau', ' Guyana', ' Haiti', ' Heard Island and McDonald Islands', ' Honduras', ' Hong Kong', ' Hungary']\n",
      "\n",
      "[' Iceland', ' India', ' Indonesia', ' Iran', ' Iraq', ' Ireland', ' Isle of Man', ' Israel', ' Italy', ' Ivory Coast']\n",
      "\n",
      "[' Jamaica', ' Japan', ' Jersey', ' Jordan', ' Kazakhstan', ' Kenya', ' Kiribati', ' Kosovo', ' Kuwait', ' Kyrgyzstan']\n",
      "\n",
      "[' Laos', ' Latvia', ' Lebanon', ' Lesotho', ' Liberia', ' Libya', ' Liechtenstein', ' Lithuania', ' Luxembourg', ' Macao']\n",
      "\n",
      "[' Macedonia', ' Madagascar', ' Malawi', ' Malaysia', ' Maldives', ' Mali', ' Malta', ' Marshall Islands', ' Martinique', ' Mauritania']\n",
      "\n",
      "[' Mauritius', ' Mayotte', ' Mexico', ' Micronesia', ' Moldova', ' Monaco', ' Mongolia', ' Montenegro', ' Montserrat', ' Morocco']\n",
      "\n",
      "[' Mozambique', ' Myanmar', ' Namibia', ' Nauru', ' Nepal', ' Netherlands', ' Netherlands Antilles', ' New Caledonia', ' New Zealand', ' Nicaragua']\n",
      "\n",
      "[' Niger', ' Nigeria', ' Niue', ' Norfolk Island', ' North Korea', ' Northern Mariana Islands', ' Norway', ' Oman', ' Pakistan', ' Palau']\n",
      "\n",
      "[' Palestinian Territory', ' Panama', ' Papua New Guinea', ' Paraguay', ' Peru', ' Philippines', ' Pitcairn', ' Poland', ' Portugal', ' Puerto Rico']\n",
      "\n",
      "[' Qatar', ' Republic of the Congo', ' Reunion', ' Romania', ' Russia', ' Rwanda', ' Saint Barthelemy', ' Saint Helena', ' Saint Kitts and Nevis', ' Saint Lucia']\n",
      "\n",
      "[' Saint Martin', ' Saint Pierre and Miquelon', ' Saint Vincent and the Grenadines', ' Samoa', ' San Marino', ' Sao Tome and Principe', ' Saudi Arabia', ' Senegal', ' Serbia', ' Serbia and Montenegro']\n",
      "\n",
      "[' Seychelles', ' Sierra Leone', ' Singapore', ' Sint Maarten', ' Slovakia', ' Slovenia', ' Solomon Islands', ' Somalia', ' South Africa', ' South Georgia and the South Sandwich Islands']\n",
      "\n",
      "[' South Korea', ' South Sudan', ' Spain', ' Sri Lanka', ' Sudan', ' Suriname', ' Svalbard and Jan Mayen', ' Swaziland', ' Sweden', ' Switzerland']\n",
      "\n",
      "[' Syria', ' Taiwan', ' Tajikistan', ' Tanzania', ' Thailand', ' Togo', ' Tokelau', ' Tonga', ' Trinidad and Tobago', ' Tunisia']\n",
      "\n",
      "[' Turkey', ' Turkmenistan', ' Turks and Caicos Islands', ' Tuvalu', ' U.S. Virgin Islands', ' Uganda', ' Ukraine', ' United Arab Emirates', ' United Kingdom', ' United States']\n",
      "\n",
      "[' United States Minor Outlying Islands', ' Uruguay', ' Uzbekistan', ' Vanuatu', ' Vatican', ' Venezuela', ' Vietnam', ' Wallis and Futuna', ' Western Sahara', ' Yemen']\n",
      "\n",
      "[' Zambia', ' Zimbabwe']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        print(response.css('div#results div a::text').extract())\n",
    "        print('')\n",
    "        \n",
    "        if response.css('div#pagination a::text').extract_first() == \"Next >\":\n",
    "            next_page = response.css('div#pagination a::attr(href)').extract_first()\n",
    "        else:\n",
    "            next_page = response.css('div#pagination a::attr(href)')[1].extract()\n",
    "        \n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "   \n",
    "\n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes soll neben dem Namen der Länder auch die Fläche, Einwohnerzahl und die Hauptstadt zu den Ländern der ersten Seite ausgegeben werden. Dazu muss das obere Programm folgendermaßen abgeändert werden. Natürlich könnten wir auch die Informationen der Länder aller Seiten ausgeben, indem wir dem \"next >\"-Link folgen. Wegen der Übersicht lassen wir dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land: Antigua and Barbuda\n",
      "Fläche: 443 square kilometres\n",
      "Einwohner: 86,754\n",
      "Hauptstadt: St. John's\n",
      "\n",
      "Land: Antarctica\n",
      "Fläche: 14,000,000 square kilometres\n",
      "Einwohner: 0\n",
      "Hauptstadt: .aq\n",
      "\n",
      "Land: Anguilla\n",
      "Fläche: 102 square kilometres\n",
      "Einwohner: 13,254\n",
      "Hauptstadt: The Valley\n",
      "\n",
      "Land: Angola\n",
      "Fläche: 1,246,700 square kilometres\n",
      "Einwohner: 13,068,161\n",
      "Hauptstadt: Luanda\n",
      "\n",
      "Land: Andorra\n",
      "Fläche: 468 square kilometres\n",
      "Einwohner: 84,000\n",
      "Hauptstadt: Andorra la Vella\n",
      "\n",
      "Land: American Samoa\n",
      "Fläche: 199 square kilometres\n",
      "Einwohner: 57,881\n",
      "Hauptstadt: Pago Pago\n",
      "\n",
      "Land: Algeria\n",
      "Fläche: 2,381,740 square kilometres\n",
      "Einwohner: 34,586,184\n",
      "Hauptstadt: Algiers\n",
      "\n",
      "Land: Albania\n",
      "Fläche: 28,748 square kilometres\n",
      "Einwohner: 2,986,952\n",
      "Hauptstadt: Tirana\n",
      "\n",
      "Land: Aland Islands\n",
      "Fläche: 1,580 square kilometres\n",
      "Einwohner: 26,711\n",
      "Hauptstadt: Mariehamn\n",
      "\n",
      "Land: Afghanistan\n",
      "Fläche: 647,500 square kilometres\n",
      "Einwohner: 29,121,286\n",
      "Hauptstadt: Kabul\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        urls = response.css('div#results div a::attr(href)').extract()\n",
    "        for url in urls:\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url=url, callback=self.parse_details)\n",
    "            \n",
    "    def parse_details(self, response):\n",
    "        \n",
    "        print(\"Land:\", response.css('td.w2p_fw::text')[3].extract())\n",
    "        print(\"Fläche:\", response.css('td.w2p_fw::text')[0].extract())\n",
    "        print(\"Einwohner:\", response.css('td.w2p_fw::text')[1].extract())\n",
    "        print(\"Hauptstadt:\", response.css('td.w2p_fw::text')[4].extract())\n",
    "        print('')\n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Möglichkeit 2: xpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath_Ausdrücke sind zwar nicht so populär wie CSS-Selektoren, bieten aber mehr Leistungen, da sie neben dem Navigieren der Struktur auch den Inhalt betrachten können. Mit XPath kann man Dinge auswählen wie: Wählen Sie den Link, der den Text \"Nächste Seite\" enthält. Dadurch wird das Scraping viel einfacher [3]. Sehen wir uns das Beispiel aus Zelle X noch einmal an. Diesmal soll aber xpath statt css genutzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land: Antigua and Barbuda\n",
      "Fläche: 443 square kilometres\n",
      "Einwohner: 86,754\n",
      "Hauptstadt: St. John's\n",
      "\n",
      "Land: Antarctica\n",
      "Fläche: 14,000,000 square kilometres\n",
      "Einwohner: 0\n",
      "Hauptstadt: .aq\n",
      "\n",
      "Land: Anguilla\n",
      "Fläche: 102 square kilometres\n",
      "Einwohner: 13,254\n",
      "Hauptstadt: The Valley\n",
      "\n",
      "Land: Angola\n",
      "Fläche: 1,246,700 square kilometres\n",
      "Einwohner: 13,068,161\n",
      "Hauptstadt: Luanda\n",
      "\n",
      "Land: Andorra\n",
      "Fläche: 468 square kilometres\n",
      "Einwohner: 84,000\n",
      "Hauptstadt: Andorra la Vella\n",
      "\n",
      "Land: American Samoa\n",
      "Fläche: 199 square kilometres\n",
      "Einwohner: 57,881\n",
      "Hauptstadt: Pago Pago\n",
      "\n",
      "Land: Algeria\n",
      "Fläche: 2,381,740 square kilometres\n",
      "Einwohner: 34,586,184\n",
      "Hauptstadt: Algiers\n",
      "\n",
      "Land: Albania\n",
      "Fläche: 28,748 square kilometres\n",
      "Einwohner: 2,986,952\n",
      "Hauptstadt: Tirana\n",
      "\n",
      "Land: Aland Islands\n",
      "Fläche: 1,580 square kilometres\n",
      "Einwohner: 26,711\n",
      "Hauptstadt: Mariehamn\n",
      "\n",
      "Land: Afghanistan\n",
      "Fläche: 647,500 square kilometres\n",
      "Einwohner: 29,121,286\n",
      "Hauptstadt: Kabul\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    start_urls = ['http://example.webscraping.com/']\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        urls = response.xpath('//div[@id=\"results\"]//a/@href').extract()\n",
    "        for url in urls:\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url=url, callback=self.parse_details)\n",
    "            \n",
    "    def parse_details(self, response):\n",
    "        \n",
    "        print(\"Land:\", response.xpath('//td[@class=\"w2p_fw\"]/text()')[3].extract())\n",
    "        print(\"Fläche:\", response.xpath('//td[@class=\"w2p_fw\"]/text()')[0].extract())\n",
    "        print(\"Einwohner:\", response.xpath('//td[@class=\"w2p_fw\"]/text()')[1].extract())\n",
    "        print(\"Hauptstadt:\", response.xpath('//td[@class=\"w2p_fw\"]/text()')[4].extract())\n",
    "        print('')\n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Sehen ist, dass man Tags bestimmter Klassen und ids suchen kann. Mit text() wird der Text zwischen den Tags extrahiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Fähigkeit von Scrapy ist die Interaktion mit Suchfunktionen und extrahieren von Daten aus Dateien im json-Format[2]. Das folgende Beispiel sucht die ersten zehn Länder mit einem G im Namen und gibt die Ergebnisse im json-Format aus. Dazu nutzt es die Suchfunktion der Seite [example.webscraping.com](http://example.webscraping.com/places/default/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"records\": [{\"pretty_link\": \"<div><a href=\\\"/places/default/view/Bulgaria-36\\\"><img src=\\\"/places/static/images/flags/bg.png\\\" /> Bulgaria</a></div>\", \"country\": \"Bulgaria\", \"id\": 2185884}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Democratic-Republic-of-the-Congo-59\\\"><img src=\\\"/places/static/images/flags/cd.png\\\" /> Democratic Republic of the Congo</a></div>\", \"country\": \"Democratic Republic of the Congo\", \"id\": 2185907}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Egypt-66\\\"><img src=\\\"/places/static/images/flags/eg.png\\\" /> Egypt</a></div>\", \"country\": \"Egypt\", \"id\": 2185914}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Equatorial-Guinea-68\\\"><img src=\\\"/places/static/images/flags/gq.png\\\" /> Equatorial Guinea</a></div>\", \"country\": \"Equatorial Guinea\", \"id\": 2185916}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/French-Guiana-77\\\"><img src=\\\"/places/static/images/flags/gf.png\\\" /> French Guiana</a></div>\", \"country\": \"French Guiana\", \"id\": 2185925}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Gabon-80\\\"><img src=\\\"/places/static/images/flags/ga.png\\\" /> Gabon</a></div>\", \"country\": \"Gabon\", \"id\": 2185928}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Gambia-81\\\"><img src=\\\"/places/static/images/flags/gm.png\\\" /> Gambia</a></div>\", \"country\": \"Gambia\", \"id\": 2185929}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Georgia-82\\\"><img src=\\\"/places/static/images/flags/ge.png\\\" /> Georgia</a></div>\", \"country\": \"Georgia\", \"id\": 2185930}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Germany-83\\\"><img src=\\\"/places/static/images/flags/de.png\\\" /> Germany</a></div>\", \"country\": \"Germany\", \"id\": 2185931}, {\"pretty_link\": \"<div><a href=\\\"/places/default/view/Ghana-84\\\"><img src=\\\"/places/static/images/flags/gh.png\\\" /> Ghana</a></div>\", \"country\": \"Ghana\", \"id\": 2185932}], \"num_pages\": 6, \"error\": \"\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    api_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "    start_urls = [api_url.format(1, 'G')]\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        print(response.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die gewünschten Daten daraus zu extrahieren muss json importiert werden und in der parse-Methode extrhiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulgaria\n",
      "Democratic Republic of the Congo\n",
      "Egypt\n",
      "Equatorial Guinea\n",
      "French Guiana\n",
      "Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "import json\n",
    "\n",
    "class CountrySpider(scrapy.Spider):\n",
    "    name = 'country'\n",
    "    api_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "    start_urls = [api_url.format(1, 'G')]\n",
    "    allowed_domains = ['example.webscraping.com']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.download_delay = 5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        for country in data['records']:\n",
    "            print(country['country'])\n",
    "        \n",
    "        \n",
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(CountrySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen:\n",
    "- [1] https://www.techopedia.com/definition/5212/web-scraping Letzter Aufruf am 04.06.2018\n",
    "- [2] Richard Lawson. Web Scraping with Python. 2015.\n",
    "- [3] https://doc.scrapy.org/en/latest/intro/tutorial.html letzter Aufruf am 04.06.2018\n",
    "- [4] https://doc.scrapy.org/en/latest/topics/spiders.html#topics-spiders letzter Aufruf 05.06.2018\n",
    "- [5] https://doc.scrapy.org/en/latest/topics/practices.html?highlight=crawlerRunner letzter Aufruf 05.06.2018\n",
    "- [6] https://doc.scrapy.org/en/latest/topics/selectors.html#topics-selectors letzter Aufruf 05.06.2018\n",
    "- [7] https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html letzter Aufruf 05.06.2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
